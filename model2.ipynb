{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import *\n",
    "import tensorboard\n",
    "\n",
    "\n",
    "# import package\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNet(nn.Module):\n",
    "    def __init__(self, input_size,num_classes, hidden_size = 16, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = True\n",
    "        self.rnn = torch.nn.LSTM(self.input_size, self.hidden_size, dropout = 0, batch_first = True, bidirectional = self.bidirectional)\n",
    "        self.linear3 = nn.Linear(16*(int(self.bidirectional)+1), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (hidden_state, cell_state) = self.rnn(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // r),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels // r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "\n",
    "# Depthwise Separable Convolution\n",
    "class Depthwise(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, stride=stride, padding=1, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "\n",
    "        self.seblock = SEBlock(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.seblock(x) * x\n",
    "        return x\n",
    "\n",
    "\n",
    "# BasicConv2d\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV1\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, width_multiplier, num_classes, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.init_weights=init_weights\n",
    "        alpha = width_multiplier\n",
    "        \n",
    "        # input_size,num_classes, hidden_size = 16, init_weights=True\n",
    "        self.lstm = RNNNet()\n",
    "\n",
    "        self.conv1 = BasicConv2d(3, int(32*alpha), 3, stride=2, padding=1)\n",
    "        self.conv2 = Depthwise(int(32*alpha), int(64*alpha), stride=1)\n",
    "        # down sample\n",
    "        self.conv3 = nn.Sequential(\n",
    "            Depthwise(int(64*alpha), int(128*alpha), stride=2),\n",
    "            Depthwise(int(128*alpha), int(128*alpha), stride=1)\n",
    "        )\n",
    "        # down sample\n",
    "        self.conv4 = nn.Sequential(\n",
    "            Depthwise(int(128*alpha), int(256*alpha), stride=2),\n",
    "            Depthwise(int(256*alpha), int(256*alpha), stride=1)\n",
    "        )\n",
    "        # down sample\n",
    "        self.conv5 = nn.Sequential(\n",
    "            Depthwise(int(256*alpha), int(512*alpha), stride=2),\n",
    "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
    "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
    "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
    "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
    "            Depthwise(int(512*alpha), int(512*alpha), stride=1),\n",
    "        )\n",
    "        # down sample\n",
    "        self.conv6 = nn.Sequential(\n",
    "            Depthwise(int(512*alpha), int(1024*alpha), stride=2)\n",
    "        )\n",
    "        # down sample\n",
    "        self.conv7 = nn.Sequential(\n",
    "            Depthwise(int(1024*alpha), int(1024*alpha), stride=2)\n",
    "        )\n",
    "        # linear\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(int(1024*alpha), 256)\n",
    "        #self.linear2 = nn.Linear( 256, 16)\n",
    "\n",
    "\n",
    "        # lstm\n",
    "        self.input_size = 256\n",
    "        self.hidden_size = 16\n",
    "        self.bidirectional = True\n",
    "        self.rnn = torch.nn.LSTM(self.input_size, self.hidden_size, dropout = 0, batch_first = True, bidirectional = self.bidirectional)\n",
    "        self.linear3 = nn.Linear(16*(int(self.bidirectional)+1), num_classes)\n",
    "        \n",
    "        # weights initialization\n",
    "        if self.init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        x = img\n",
    "        x_ = text\n",
    "        # input_size,num_classes, hidden_size = 16, init_weights=True\n",
    "        x_ = self.lstm()\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        #x = self.linear2(x)\n",
    "        x, (hidden_state, cell_state) = self.rnn(x)\n",
    "        x = self.linear3(x)\n",
    "        #return torch.argmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    # weights initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def mobilenet(alpha=1, num_classes=1):\n",
    "    return MobileNet(alpha, num_classes)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m losss \u001b[39m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m RNNNet(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mToTensor(),])\n\u001b[0;32m      7\u001b[0m dataset \u001b[39m=\u001b[39m KwenDataset(path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m, transform\u001b[39m=\u001b[39m transform, lstm \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'num_classes'"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "losss = {}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNNet(1).to(device)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "dataset = KwenDataset(path = 'dataset', transform= transform, lstm = True)\n",
    "dataloader =DataLoader(dataset=dataset,batch_size=32,shuffle=True,drop_last=False)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "learning_rate = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "decive = 'cuda'\n",
    "count_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
